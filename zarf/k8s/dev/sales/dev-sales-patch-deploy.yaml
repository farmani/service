apiVersion: apps/v1
kind: Deployment
metadata:
  name: sales
  namespace: sales-system

spec:
  selector:
    matchLabels:
      app: sales

  replicas: 1

  strategy:
    type: Recreate

  template:
    metadata:
      labels:
        app: sales

    spec:
      dnsPolicy: ClusterFirstWithHostNet
      hostNetwork: true

      # Compute Resource Quotas
      #
      # The request value helps K8s determine what node a POD can be run on by making
      # sure the number of requesting cores for the sum of all PODs are never greater
      # than the number of cores on a given node. If you have 8 PODs each requesting 1
      # core (1000m), then a node of 8 cores can run those 8 PODs. If you have 16 PODs
      # each requesting 1/2 core (500m), then a node of 8 cores can run those 16 PODs.
      #
      # The limit value helps the container runtime to determine which containers
      # (configured in a POD running on the node) can use a CPU and for how long. This
      # is measured in time where 100ms represents a unit of execution time. If a limit
      # of 1000m is requested, that means the container wants the full 100ms of time
      # all the time. There is no CPU affinity, but you can imagine the container is
      # given a full CPU to use on their own. A limit of 500m means the container wants
      # 50ms of the 100ms per cycle. You can imagine it gets to share a CPU half the
      # time with some other container.
      #
      # There are two points of view on these CPU quotas.
      #
      # Set the request to match the limit. The idea is if the container is requesting
      # 1/2 a CPU, then limit the execution time to 1/2 a CPU as well. This balances
      # the amount of CPU on the node with the amount of execution time. The drawback
      # is the container can't handle a burst of traffic because it might have used
      # it's allotted time and now it has to wait for the next 100ms cycle.
      #
      # Don't set any limit. The idea is allow all the containers to use the full
      # capacity of CPU's on the node it's running on. So if there are 8 CPUs on the
      # node, allow every program to use all 8 CPUs at the same time. This won't limit
      # a container from getting the CPU it needs, outside of competing with the other
      # containers.
      #
      # Determining the strategy depends on how you are measuring the performance of
      # the containers to determine when a new POD needs to be created on a different
      # node to handle the perceived load. Clusters have multiple dimensions of
      # autoscaling: Horizontal Pod Autoscaling, Vertical Pod Autoscaling, Autoscaling
      # of Nodes within a Node Group, Autoscaling of Node Groups themselves.

      # Nutty Swiss
      #
      # I’ve naively put jobs into roughly two buckets. The ones where access to CPU
      # cycles matter (CPU bound) and the ones where throttling is ok (IO Bound). For
      # CPU bound work, toss the Go program a decent number of cores, but not too much.
      # Allow for mobility of tasks within the cluster. For the IO bound work, toss
      # the Go program about a core, maybe less.
      #
      # In both cases, give the Go runtime a core and set your limit to (limit += 1)
      # since your Go program is CPU bound. Then let autoscaling take effect.
      #
      # Ideally you’d give tasks a reservation and no limit. However, I’ve never seen
      # anyone actually write software that deals with the situation where the job all
      # of a sudden does not get the “usual” over reservation cycles anymore. A
      # situation that usually happens during load spikes. Or outages, etc. Leave the
      # burst capable options to batch jobs.
      #
      # In the end, your tasks are a small player in a bigger game. Lifting the whole,
      # enabling the whole, is usually more important than optimizing just your workload.
      # Hence the 1/3 to 1/4 max core counts.

      # Nick Stogner
      #
      # Requests are separate from Limits b/c setting requests lower than limits allows
      # for more efficient resource utilization when you start packing multiple PODs
      # onto a given Node. Limits are there to prevent noisy neighbor conditions with
      # collocated PODs.

      # Resource Requests:m
      #
      # Purpose: Resource requests are used to specify the minimum amount of resources that a container needs to run. Kubernetes uses this information to decide where to place the Pod based on available resources within the cluster.
      # Example: If you specify a CPU request of 0.5 CPU cores and a memory request of 256Mi (mebibytes), Kubernetes will ensure that the Pod is scheduled to a node with at least these resources available. This prevents overloading nodes with containers that require more resources than are available.
      # Impact: Resource requests affect the initial placement and scheduling of Pods within the cluster. Kubernetes will only place a Pod on a node if the node has enough available resources to satisfy the Pod's request.
      #
      # Resource Limits:
      #
      # Purpose: Resource limits are used to specify the maximum amount of resources that a container is allowed to consume. This is a safety mechanism to prevent containers from monopolizing resources on a node and potentially causing performance problems for other containers on the same node.
      # Example: If you set a CPU limit of 1 CPU core and a memory limit of 512Mi for a container, it means that the container can consume up to these limits but will not be allowed to exceed them. If the container attempts to use more resources than its limit, Kubernetes will throttle or terminate the container.
      # Impact: Resource limits affect the behavior of containers once they are running. They ensure that a container cannot use more resources than it has been allocated, helping to maintain cluster stability and prevent resource contention.
      # In summary, resource requests are used for scheduling and placement decisions, ensuring that Pods are placed on nodes with sufficient resources to meet their requirements. Resource limits, on the other hand, enforce constraints on how much CPU and memory a container can actually consume, preventing resource hogging and ensuring fair resource distribution within the cluster.
      # It's important to configure both resource requests and limits appropriately for your containers to ensure efficient resource utilization and cluster stability. Failing to set reasonable values for these parameters can lead to performance issues and resource contention in your Kubernetes cluster.

      containers:
        - name: sales-api
          resources:
            requests:
              cpu: "1500m" # I need access to 1+1/2 core on the node.
              memory: "500Mi"
            limits:
              cpu: "1500m" # Execute instructions 150ms/200ms on my 1 core.
              memory: "500Mi"
